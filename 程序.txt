## 基于gym的强化学习，代码分为gym学习环境，DDPG学习代码及DQN学习代码
## gym学习环境
import gym
from gym import spaces
from gym.utils import seeding
import numpy as np


def deg_(x, y):  # 将输入坐标转化为360度角度制
    if x == 0:
        res = (90 + np.where(y > 0, 0, 180)) * np.where(y == 0, 0, 1)
    else:
        res = np.degrees(np.arctan(y / x)) + np.where(x > 0, 0, 180) + np.where(x > 0 and y < 0, 360, 0)
    return res

def dis_(x, y, dx, dy): #计算两点间的距离
    return np.sqrt((x-dx)**2+(y-dy)**2)

class myEnv2(gym.Env):
    metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}

    def __init__(self):
        self.tau = 0.1  # 时间微分
        self.dog_v = 20  # 狗的速度
        self.sheep_v = 25  # 羊的速度
        self.a = 100  # 椭圆长轴
        self.b = 70  #
        self.dog1_x = 0
        self.dog2_x = 0
        self.dog1_y = self.b
        self.dog2_y = -self.b
        self.sheep_fx = 0
        self.sheep_fy = 0  # 羊的初始坐标
        self.sheep_col = 3.5  # 羊与狗的碰撞半径
        self.action_space = spaces.Box(low=0, high=360, shape=(1,), dtype=np.float32)
        high = np.array([self.a*2, self.b*2, 1], dtype=np.float32)
        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)
        self.seed()
        self.viewer = None
        self.state = None
        self.steps_beyond_done = None

    def reset(self):
        self.state = self.np_random.uniform(low=-70 / 2, high=70 / 2,size=(3,))  # 在重置游戏时，随机羊的出生点
        self.action_space = spaces.Box(low=0, high=360, shape=(1,), dtype=np.float32)
        self.dog1_x = 0
        self.dog2_x = 0
        self.dog1_y = self.b
        self.dog2_y = -self.b
        self.steps_beyond_done = None
        return np.array(self.state)

    def step(self, action):
        err_msg = "%r (%s) invalid" % (action, type(action))
        if not self.action_space.contains(action):
            action = self.action_space.low + np.random.rand()*(self.action_space.high-self.action_space.low)
        assert self.action_space.contains(action), err_msg
        x, y, z = self.state
        theta = deg_(x, y)
        self.action_space = spaces.Box(low=theta - 90, high=theta + 90, shape=(1,), dtype=np.float32)
        x, y, z = self.state  # 获取羊的坐标
        x = x + np.cos(np.radians(action)) * self.tau * self.sheep_v
        y = y + np.sin(np.radians(action)) * self.tau * self.sheep_v  # 根据速度和动作变动羊的坐标
        self.state = (x, y)
        dog1_theta = deg_(self.dog1_x, self.dog1_y)
        dog2_theta = deg_(self.dog2_x, self.dog2_y)
        dir1 = np.where(180 >= dog1_theta - theta >= 0 or -180 >= dog1_theta - theta >= -360, -1, 1)
        dir2 = np.where(180 >= dog2_theta - theta >= 0 or -180 >= dog2_theta - theta >= -360, -1, 1)
        if (dis_(x,y,self.dog1_x, self.dog1_y) <= dis_(x,y,self.dog2_x, self.dog2_y)):
            self.dog1_x = self.dog1_x + np.cos(np.radians(dog1_theta + 90 * dir1)) * self.tau * self.dog_v
            self.dog1_y = self.dog1_y + np.sin(np.radians(dog1_theta + 90 * dir1)) * self.tau * self.dog_v
            self.dog2_x = self.dog2_x + np.cos(np.radians(dog2_theta + 90 * (-dir1))) * self.tau * self.dog_v
            self.dog2_y = self.dog2_y + np.sin(np.radians(dog2_theta + 90 * (-dir1))) * self.tau * self.dog_v
        else:
            self.dog1_x = self.dog1_x + np.cos(np.radians(dog1_theta + 90 * (-dir2))) * self.tau * self.dog_v
            self.dog1_y = self.dog1_y + np.sin(np.radians(dog1_theta + 90 * (-dir2))) * self.tau * self.dog_v
            self.dog2_x = self.dog2_x + np.cos(np.radians(dog2_theta + 90 * dir2)) * self.tau * self.dog_v
            self.dog2_y = self.dog2_y + np.sin(np.radians(dog2_theta + 90 * dir2)) * self.tau * self.dog_v
        k1 = self.dog1_y / self.dog1_x
        if self.dog1_x >= 0:
            self.dog1_x = np.sqrt(1 / ((1 / self.a ** 2) + (k1 ** 2 / self.b ** 2)))
            self.dog1_y = k1 * self.dog1_x
        else:
            self.dog1_x = -np.sqrt(1 / ((1 / self.a ** 2) + (k1 ** 2 / self.b ** 2)))
            self.dog1_y = k1 * self.dog1_x

        k2 = self.dog2_y / self.dog2_x
        if self.dog2_x >= 0:
            self.dog2_x = np.sqrt(1 / ((1 / self.a ** 2) + (k2 ** 2 / self.b ** 2)))
            self.dog2_y = k2 * self.dog2_x
        else:
            self.dog2_x = -np.sqrt(1 / ((1 / self.a ** 2) + (k2 ** 2 / self.b ** 2)))
            self.dog2_y = k2 * self.dog2_x

        done = bool(x*x/(self.a*self.a)+y*y/(self.b*self.b) >= 1)
        if not done:
            reward = -1
        else:
            reward = 300
        if np.sqrt((x - self.dog1_x) ** 2 + (y - self.dog1_y) ** 2) <= self.sheep_col or np.sqrt((x - self.dog2_x) ** 2 + (y - self.dog2_y) ** 2) <= self.sheep_col:
            reward = -300
            done = True
        self.state = np.array([int(self.state[0]), int(self.state[1]), 0])
        return self.state, reward, done, {}

    def render(self, mode='human'):  # 渲染引擎
        screen_width = 500
        screen_height = 500

        from gym.envs.classic_control import rendering

        if self.viewer is None:
            self.viewer = rendering.Viewer(screen_width, screen_height)
            self.middle_trans = rendering.Transform(translation=(screen_width / 2, screen_height / 2))
            self.sheep = rendering.make_circle(3)
            self.sheeptrans = rendering.Transform()
            self.sheep.add_attr(self.sheeptrans)
            self.sheep.set_color(1, 0, 0)
            self.viewer.add_geom(self.sheep)
            self.cir_point = rendering.make_circle(2)
            self.cir_point.add_attr(self.middle_trans)
            self.viewer.add_geom(self.cir_point)
            self.dog1 = rendering.make_circle(3)
            self.dog1trans = rendering.Transform()
            self.dog1.add_attr(self.dog1trans)
            self.dog1.set_color(0, 0, 1)
            self.viewer.add_geom(self.dog1)
            self.dog2 = rendering.make_circle(3)
            self.dog2trans = rendering.Transform()
            self.dog2.add_attr(self.dog2trans)
            self.dog2.set_color(0, 0, 1)
            self.viewer.add_geom(self.dog2)
        if self.state is None:
            return None
        x, y, z = self.state
        self.sheeptrans.set_translation(x + screen_width / 2, y + screen_height / 2)
        self.dog1trans.set_translation(self.dog1_x + screen_width / 2, self.dog1_y + screen_height / 2)
        self.dog2trans.set_translation(self.dog2_x + screen_width / 2, self.dog2_y + screen_height / 2)
        return self.viewer.render(return_rgb_array=mode == 'rgb_array')

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def close(self):
        if self.viewer:
            self.viewer.close()
            self.viewer = None


## DDPG强化学习代码
import tensorflow._api.v2.compat.v1 as tf
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.disable_v2_behavior()
from job.myEnv2 import myEnv2
import numpy as np
import time

MAX_EPISODES = 500
MAX_EP_STEPS = 200
LR_A = 0.001  # learning rate for actor
LR_C = 0.002  # learning rate for critic
GAMMA = 0.9  # reward discount
TAU = 0.01  # soft replacement
MEMORY_CAPACITY = 10000
BATCH_SIZE = 32
RENDER = True


class DDPG(object):
    def __init__(self, a_dim, s_dim, a_bound, ):
        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)
        self.pointer = 0
        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound
        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')
        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')
        self.R = tf.placeholder(tf.float32, [None, 1], 'r')

        with tf.variable_scope('Actor'):
            self.a = self._build_a(self.S, scope='eval', trainable=True)
            a_ = self._build_a(self.S_, scope='target', trainable=False)

        with tf.variable_scope('Critic'):
            # assign self.a = a in memory when calculating q for td_error,
            # otherwise the self.a is from Actor when updating Actor
            q = self._build_c(self.S, self.a, scope='eval', trainable=True)
            q_ = self._build_c(self.S_, a_, scope='target', trainable=False)

        # networks parameters
        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')
        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')
        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')
        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')

        # target net replacement
        self.soft_replace = [tf.assign(t, (1 - TAU) * t + TAU * e) for t, e in
                             zip(self.at_params + self.ct_params, self.ae_params + self.ce_params)]
        q_target = self.R + GAMMA * q_  # in the feed_dic for the td_error, the self.a should change to actions in memory
        td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q)
        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, var_list=self.ce_params)
        a_loss = - tf.reduce_mean(q)  # maximize the q
        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.ae_params)
        self.saver = tf.train.Saver()
        self.sess = tf.InteractiveSession()
        self.sess.run(tf.global_variables_initializer())
        checkpoint = tf.train.get_checkpoint_state("saved_networks")
        if checkpoint and checkpoint.model_checkpoint_path:
            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)
            print("Successfully loaded:", checkpoint.model_checkpoint_path)
        else:
            print("Could not find old network weights")

    def choose_action(self, s):
        return self.sess.run(self.a, {self.S: s[np.newaxis, :]})[0]

    def learn(self):  # soft target replacement
        self.sess.run(self.soft_replace)
        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)
        bt = self.memory[indices, :]
        bs = bt[:, :self.s_dim]
        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]
        br = bt[:, -self.s_dim - 1: -self.s_dim]
        bs_ = bt[:, -self.s_dim:]

        self.sess.run(self.atrain, {self.S: bs})
        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})

    def store_transition(self, s, a, r, s_):
        transition = np.hstack((s, a, [r], s_))

        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory
        self.memory[index, :] = transition
        self.pointer += 1

    def _build_a(self, s, scope, trainable):
        with tf.variable_scope(scope):
            net = tf.layers.dense(s, 30, activation=tf.nn.relu, name='l1', trainable=trainable)
            a = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, name='a', trainable=trainable)
            return tf.multiply(a, self.a_bound, name='scaled_a')

    def _build_c(self, s, a, scope, trainable):
        with tf.variable_scope(scope):
            n_l1 = 30
            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable)
            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable)
            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)
            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)
            return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)


if __name__ == "__main__":
    env = myEnv2()
    env = env.unwrapped
    env.seed(1)
    s_shape = env.observation_space.shape[0]
    a_shape = env.action_space.shape[0]
    a_bound = env.action_space.high
    ddpg = DDPG(a_shape, s_shape, a_bound)
    var = 3  # control exploration
    score_list = []
    t1 = time.time()
    for i in range(MAX_EPISODES):
        s = env.reset()
        ep_reward = 0
        for j in range(MAX_EP_STEPS):
            if RENDER:
                env.render()
            # Add exploration noise
            lb = env.action_space.low
            hb = env.action_space.high
            a = ddpg.choose_action(s)
            a = np.clip(np.random.normal(a, var), lb + 35,
                        hb - 35)  # add randomness to action selection for exploration
            s_, r, done, info = env.step(a)
            ddpg.store_transition(s, a, r / 10, s_)
            if ddpg.pointer > MEMORY_CAPACITY:
                var *= .9995  # decay the action randomness
                ddpg.learn()
            s = s_
            ep_reward += r
            if j == MAX_EP_STEPS - 1 or done:
                print('Episode:', i, ' Reward: %i' % int(ep_reward), 'Explore: %.2f' % var, )
                score_list.append(ep_reward)
                break
        if i == MAX_EPISODES - 1:
            ddpg.saver.save(ddpg.sess, 'saved_networks/' + 'network' + '-dqn', global_step=ddpg.pointer)

    print('ddpg Running time: ', time.time() - t1)

    import pandas as pd

    test = pd.DataFrame(data=score_list)
    test.to_csv('ddpg_output.csv', encoding='utf-8')


## DQN强化学习代码
import random
import time
from collections import deque
import numpy as np
from tensorflow.keras import models, layers, optimizers

from job.myEnv2 import myEnv2


def deg_(x, y):  # 将输入坐标转化为360度角度制
    if x == 0:
        res = (90 + np.where(y > 0, 0, 180)) * np.where(y == 0, 0, 1)
    else:
        res = np.degrees(np.arctan(y / x)) + np.where(x > 0, 0, 180) + np.where(x > 0 and y < 0, 360, 0)
    return res


class DQN(object):
    def __init__(self):
        self.step = 0
        self.update_freq = 200  # 模型更新频率
        self.replay_size = 2000  # 训练集大小
        self.replay_queue = deque(maxlen=self.replay_size)
        self.model = self.create_model()
        self.target_model = self.create_model()

    def create_model(self):
        """创建一个隐藏层为100的神经网络"""
        STATE_DIM, ACTION_DIM = 3, 3
        model = models.Sequential([
            layers.Dense(100, input_dim=STATE_DIM, activation='relu'),
            layers.Dense(ACTION_DIM, activation="linear")
        ])
        model.compile(loss='mean_squared_error',
                      optimizer=optimizers.Adam(0.001))
        return model

    def act(self, s, epsilon=0.1):
        """预测动作"""
        # 刚开始时，加一点随机成分，产生更多的状态
        choice = [-45, 0, 45]
        theta = deg_(s[0], s[1])
        ans = []
        if np.random.uniform() < epsilon - self.step * 0.0002:
            ans.append(choice[(np.random.choice([0, 1, 2]))] + theta)
            return ans
        index = np.argmax(self.model.predict(np.array([s])))
        ans.append(theta + choice[index])
        return ans

    def save_model(self, file_path='dqn.h5'):
        print('model saved')
        self.model.save(file_path)

    def remember(self, s, a, next_s, reward):
        """历史记录，position >= 0.4时给额外的reward，快速收敛"""
        if next_s[0] >= 0.4:
            reward += 1
        self.replay_queue.append((s, a, next_s, reward))

    def train(self, batch_size=64, lr=1, factor=0.95):
        if len(self.replay_queue) < self.replay_size:
            return
        self.step += 1
        # 每 update_freq 步，将 model 的权重赋值给 target_model
        if self.step % self.update_freq == 0:
            self.target_model.set_weights(self.model.get_weights())

        replay_batch = random.sample(self.replay_queue, batch_size)
        s_batch = np.array([replay[0] for replay in replay_batch])
        next_s_batch = np.array([replay[2] for replay in replay_batch])

        Q = self.model.predict(s_batch)
        Q_next = self.target_model.predict(next_s_batch)

        # 使用公式更新训练集中的Q值
        for i, replay in enumerate(replay_batch):
            alist, reward, _, _ = replay
            a = np.argmax(alist)
            Q[i][a] = (1 - lr) * Q[i][a] + lr * (reward + factor * np.amax(Q_next[i]))

        # 传入网络进行训练
        self.model.fit(s_batch, Q, verbose=0)


if __name__ == '__main__':
    env = myEnv2()
    episodes = 500  # 训练500次
    score_list = []  # 记录所有分数
    agent = DQN()
    t1 = time.time()
    for i in range(episodes):
        s = env.reset()
        score = 0
        while True:
            env.render()
            a = agent.act(s)
            next_s, reward, done, _ = env.step(a)
            agent.remember(s, a, next_s, reward)
            agent.train()
            score += reward
            s = next_s
            if done:
                score_list.append(score)
                print('episode:', i, 'score:', score, 'max:', max(score_list))
                break
        if i == episodes - 1:
            # agent.save_model()
            break
    env.close()
    print('dqn Running time: ', time.time() - t1)

    import pandas as pd

    test = pd.DataFrame(data=score_list)
    test.to_csv('dqn_output.csv', encoding='utf-8')


%%  topsis评分
clear;clc
load data.mat
X = table2array(data);
%%  第一步：正向化（极小型、中间型、区间型转极大型）
[n,m] = size(X);
disp(['共有' num2str(n) '个评价对象, ' num2str(m) '个评价指标']) 
X(1,1) =1/X(1,1);
X(2,1) = 1/X(2,1);
% Judge = input(['这' num2str(m) '个指标是否需要经过正向化处理，需要请输入1 ，不需要输入0：  ']);
% if Judge == 1
%     Position = input('请输入需要正向化处理的指标所在的列，例如第2、3、6三列需要处理，那么你需要输入[2,3,6]： '); %[2,3,4]
%     disp('请输入需要处理的这些列的指标类型（1：极小型， 2：中间型， 3：区间型） ')
%     Type = input('例如：第2列是极小型，第3列是区间型，第6列是中间型，就输入[1,3,2]：  '); %[2,1,3]
%         for i = 1 : size(Position,2)  %这里需要对这些列分别处理，因此我们需要知道一共要处理的次数，即循环的次数
%             X(:,Position(i)) = Positivization(X(:,Position(i)),Type(i),Position(i));
%         end
%     disp('正向化后的矩阵 X =  ')
%     disp(X)
% end
%%  第二步：对正向化矩阵标准化
Z = X ./ repmat(sum(X.*X) .^ 0.5,n,1); %标准化
disp('标准化矩阵 Z = ')
disp(Z)
%%  第三步：计算权重
W = [1/3,1/3,1/3];
disp('各成分熵权为：')
disp(W)
%%  第四步：计算与最大值和最小值的距离，并计算得分
D_P = sum((Z - repmat(max(Z),n,1)) .^ 2 .* repmat(W,n,1),2) .^ 0.5; %加权D+
D_N = sum((Z - repmat(min(Z),n,1)) .^ 2  .* repmat(W,n,1),2) .^ 0.5; %加权D-
S = D_N ./ (D_P + D_N); %未归一化的评分（加权Topsis方法）
disp('最后的得分为：')
stand_S = S ./ sum(S);
[sorted_S,index] = sort(stand_S ,'descend');
disp('标准化后得分为：')
disp(sorted_S)
disp('对应的下标为:')
disp(index)